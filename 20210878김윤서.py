# -*- coding: utf-8 -*-
"""20210878김윤서.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xOGyHCvfWQWhhu9bp5NNqQsV0VXFFVpA
"""

!pip install tensorflow

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

import numpy as np
import pandas as pd

from tensorflow import keras
from tensorflow.keras.datasets import mnist

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Conv2D, MaxPooling2D, Dense, Flatten
from tensorflow.keras.layers import Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical

from keras.callbacks import ModelCheckpoint

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/gdrive')

# mnist 데이터 불러오기

mnist = keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train.shape

y_train.shape

np.random.seed(1234)
val_ind = np.random.choice(x_train.shape[0], 10000, replace=False)
tr_ind = np.setdiff1d(np.arange(x_train.shape[0]),val_ind)

x_valid, y_valid = x_train[val_ind], y_train[val_ind]
x_train, y_train = x_train[tr_ind], y_train[tr_ind]

x_train = (x_train / 255) - 0.5
x_valid = (x_valid / 255) - 0.5
x_test = (x_test / 255) - 0.5

x_train = np.expand_dims(x_train, axis=3)
x_valid = np.expand_dims(x_valid, axis=3)
x_test = np.expand_dims(x_test, axis=3)

y_train_onehot = to_categorical(y_train, num_classes=10)
y_valid_onehot = to_categorical(y_valid, num_classes=10)
y_test_onehot = to_categorical(y_test, num_classes=10)

print(x_train.shape)
print(x_valid.shape)
print(x_test.shape)

filter_size = 3
pool_size = 2

model = Sequential([
  keras.Input(shape=(28,28,1)),
  Conv2D(filters = 30, kernel_size = (3,3), padding='same'),
  BatchNormalization(),
  Activation('relu'),
  Conv2D(filters = 30, kernel_size = (3,3), padding='same'),
  BatchNormalization(),
  Activation('relu'),
  MaxPooling2D(pool_size=pool_size),
  Conv2D(filters = 30, kernel_size = (3,3), padding='same'),
  BatchNormalization(),
  Activation('relu'),
  Conv2D(filters = 30, kernel_size = filter_size, padding='same'),
  BatchNormalization(),
  Activation('relu'),
  MaxPooling2D(pool_size=pool_size),
  Conv2D(filters = 30, kernel_size = (3,3), padding='same'),
  BatchNormalization(),
  Activation('relu'),
  Conv2D(filters = 30, kernel_size = filter_size, padding='same'),
  BatchNormalization(),
  Activation('relu'),
  MaxPooling2D(pool_size=pool_size),
  Flatten(),
  Dense(50),
  Activation('relu'),
  Dense(10, activation='softmax'),
])

model.summary()

checkpoint_filepath = "/content/gdrive/best_cnn.ckpt"
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True,
    verbose=1)

# Compile the model.
model.compile(
  'adam',
  loss='categorical_crossentropy',
  metrics=['accuracy'],
)

# Train the model.
history = model.fit(x=x_train,y=y_train_onehot,
          epochs=10,
          validation_data=(x_valid, y_valid_onehot),
          verbose=1,
          batch_size=256,
          callbacks=[model_checkpoint_callback]
)

model.load_weights(checkpoint_filepath)

model.evaluate(x_test, y_test_onehot)

"""epoch을 10으로 하고 filter의 개수를 30으로 했을 때의 accuracy는 0.9915로 높게 나오는 편이다.
epoch 6에서 한 번 accuracy 업데이트가 멈췄다가 epoch 8에서 한번 더 업데이트 되는 것을 확인해볼 수 있다.
"""

